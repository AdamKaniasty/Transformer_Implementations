{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_json('data/trainmodel.json')\n",
    "validate = pd.read_json('data/val.json')\n",
    "\n",
    "train['answers'] = train['answers'].apply(lambda x: x[0])\n",
    "validate['answers'] = validate['answers'].apply(lambda x: x[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.417174070Z",
     "start_time": "2024-04-07T22:12:02.377232611Z"
    }
   },
   "id": "a388de46bf01c410",
   "execution_count": 301
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         qId        answers                                              qText\n0  wqr000001  Padmé Amidala  what character did natalie portman play in sta...\n1  wqr000002  New York City                      what state does selena gomez?\n2  wqr000003        Bahamas        what country is the grand bahama island in?\n3  wqr000005    Denethor II  what character did john noble play in lord of ...\n4  wqr000006  Chicago Bulls                     who does joakim noah play for?",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qId</th>\n      <th>answers</th>\n      <th>qText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>wqr000001</td>\n      <td>Padmé Amidala</td>\n      <td>what character did natalie portman play in sta...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wqr000002</td>\n      <td>New York City</td>\n      <td>what state does selena gomez?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wqr000003</td>\n      <td>Bahamas</td>\n      <td>what country is the grand bahama island in?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wqr000005</td>\n      <td>Denethor II</td>\n      <td>what character did john noble play in lord of ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>wqr000006</td>\n      <td>Chicago Bulls</td>\n      <td>who does joakim noah play for?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.429313570Z",
     "start_time": "2024-04-07T22:12:02.417427777Z"
    }
   },
   "id": "fd83109d27cae1db",
   "execution_count": 302
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "questions = train['qText'].values\n",
    "answers = train['answers'].values\n",
    "questions_val = validate['qText'].values\n",
    "answers_val = validate['answers'].values\n",
    "\n",
    "questions_text = \" \".join(list(questions))\n",
    "answers_text = \" \".join(list(answers))\n",
    "questions_val_text = \" \".join(list(questions_val))\n",
    "answers_val_text = \" \".join(list(answers_val))\n",
    "\n",
    "full_text = questions_text + \" \" + answers_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.432941957Z",
     "start_time": "2024-04-07T22:12:02.428404550Z"
    }
   },
   "id": "25c796eafcfdf7a0",
   "execution_count": 303
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "56d610e0a443e539",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.465058153Z",
     "start_time": "2024-04-07T22:12:02.437150926Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(set(full_text))\n",
    "\n",
    "encoder_map = {c: i for i, c in enumerate(set(full_text))}\n",
    "decoder_map = {i: c for i, c in enumerate(set(full_text))}\n",
    "\n",
    "encode = lambda x: [encoder_map[c] for c in x]\n",
    "decode = lambda x: ''.join([decoder_map[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "([18, 91, 17, 41, 56, 17, 41, 3, 64, 14, 90], 'Test string')"
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"Test string\"), decode(encode(\"Test string\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.484523738Z",
     "start_time": "2024-04-07T22:12:02.442902157Z"
    }
   },
   "id": "d2d306d750fdb8c",
   "execution_count": 305
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(2115)\n",
    "batch_size = 16\n",
    "seq_len = 16\n",
    "n_embed = 32\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, seq_len):\n",
    "    padded_sequences = [seq + [0] * (seq_len - len(seq)) for seq in sequences]\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "def truncate_sequences(sequences, max_len):\n",
    "    truncated_sequences = [seq[:max_len] for seq in sequences]\n",
    "    return truncated_sequences\n",
    "\n",
    "\n",
    "def get_batches(questions, answers):\n",
    "    idx = torch.randint(0, len(questions), (batch_size,))\n",
    "    batch_questions = questions[idx].tolist()\n",
    "    batch_answers = answers[idx].tolist()\n",
    "\n",
    "    encoded_questions = [encode(q)[:-1] for q in batch_questions]\n",
    "    encoded_answers = [encode(a)[:-1] for a in batch_answers]\n",
    "\n",
    "    y_questions = [encode(q)[1:] for q in batch_questions]\n",
    "    y_answers = [encode(a)[1:] for a in batch_answers]\n",
    "\n",
    "    max_seq_len = max([len(seq) for seq in encoded_questions + encoded_answers + y_questions + y_answers])\n",
    "\n",
    "    encoded_questions = truncate_sequences(encoded_questions, seq_len)\n",
    "    encoded_answers = truncate_sequences(encoded_answers, seq_len)\n",
    "    y_questions = truncate_sequences(y_questions, seq_len)\n",
    "    y_answers = truncate_sequences(y_answers, seq_len)\n",
    "\n",
    "    x = pad_sequences(encoded_questions, max_seq_len)\n",
    "    y = pad_sequences(y_questions, max_seq_len)\n",
    "    x_ans = pad_sequences(encoded_answers, max_seq_len)\n",
    "    y_ans = pad_sequences(y_answers, max_seq_len)\n",
    "\n",
    "    x, y, x_ans, y_ans = map(lambda seqs: torch.tensor(seqs), [x, y, x_ans, y_ans])\n",
    "\n",
    "    return x, y, x_ans, y_ans\n",
    "\n",
    "\n",
    "x, y, x_ans, y_ans = get_batches(questions, answers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.484972131Z",
     "start_time": "2024-04-07T22:12:02.465361083Z"
    }
   },
   "id": "38603cf7231ae894",
   "execution_count": 306
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class MaskedHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        self.head_size = head_size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-1, -2) * C ** -0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        return weights @ v\n",
    "\n",
    "\n",
    "class HeadEncoder(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.head_size = head_size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weights = q @ k.transpose(-1, -2) * C ** -0.5\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        return weights @ v\n",
    "\n",
    "\n",
    "class HeadDecoder(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.head_size = head_size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, k, v):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        weights = q @ k.transpose(-1, -2) * C ** -0.5\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        return weights @ v"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.639246837Z",
     "start_time": "2024-04-07T22:12:02.465436916Z"
    }
   },
   "id": "7755ea6141bbbcb9",
   "execution_count": 307
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MaskedMultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([MaskedHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "\n",
    "class MultiHeadEncoder(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([HeadEncoder(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))\n",
    "\n",
    "\n",
    "class MultiHeadDecoder(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([HeadDecoder(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, k, v):\n",
    "        out = torch.cat([h(x, k, v) for h in self.heads], dim=-1)\n",
    "        return self.dropout(self.proj(out))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.639695100Z",
     "start_time": "2024-04-07T22:12:02.484675363Z"
    }
   },
   "id": "3bb13f5732a3e657",
   "execution_count": 308
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.654796403Z",
     "start_time": "2024-04-07T22:12:02.525391680Z"
    }
   },
   "id": "7631bf1ebdc37a38",
   "execution_count": 309
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BlockDecoder(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.msa_heads = MaskedMultiHead(n_head, head_size)\n",
    "        self.sa_heads = MultiHeadDecoder(n_head, head_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm3 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x, k, v):\n",
    "        x = x + self.msa_heads(self.layer_norm1(x))\n",
    "        x = x + self.sa_heads(self.layer_norm2(x), k, v)\n",
    "        x = x + self.feed_forward(self.layer_norm3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BlockEncoder(nn.Module):\n",
    "    def __init__(self, n_embed, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHeadEncoder(n_head, head_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.layer_norm1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.657926943Z",
     "start_time": "2024-04-07T22:12:02.645204817Z"
    }
   },
   "id": "3e46d2ed4d9057f6",
   "execution_count": 310
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(seq_len, n_embed)\n",
    "        self.blocks = [BlockDecoder(n_embed, num_heads) for _ in range(3)]\n",
    "        self.blocks = nn.ModuleList(self.blocks)\n",
    "        self.layer_norm = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, k, v, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embedding = self.token_embedding_table(idx)\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_embedding + position_embedding\n",
    "        for block in self.blocks:\n",
    "            x = block(x, k, v)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, n):\n",
    "        for _ in range(n):\n",
    "            idx_crop = idx[:, -seq_len:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:, -1, :]\n",
    "            p = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(p, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "        return idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.691738820Z",
     "start_time": "2024-04-07T22:12:02.663903929Z"
    }
   },
   "id": "a4c34b6342b0c540",
   "execution_count": 311
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // num_heads\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(seq_len, n_embed)\n",
    "        self.blocks = nn.Sequential(*[BlockEncoder(n_embed, num_heads, head_size) for _ in range(2)])\n",
    "        self.layer_norm = nn.LayerNorm(n_embed)\n",
    "        self.key_projection = nn.Linear(n_embed, head_size)\n",
    "        self.value_projection = nn.Linear(n_embed, head_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        token_embedding = self.token_embedding_table(idx)\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_embedding + position_embedding\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        k = self.key_projection(x)\n",
    "        v = self.value_projection(x)\n",
    "        return k, v"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.695101627Z",
     "start_time": "2024-04-07T22:12:02.692004590Z"
    }
   },
   "id": "8a1801b072224005",
   "execution_count": 312
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, prompt, response):\n",
    "        k, v = self.encoder(prompt)\n",
    "\n",
    "        x = self.decoder(response, k, v)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.706322589Z",
     "start_time": "2024-04-07T22:12:02.695627917Z"
    }
   },
   "id": "56546bda66bb7936",
   "execution_count": 313
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=200):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batches(questions, answers) if split == 'train' else get_batches(questions_val, answers_val)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:25.057998153Z",
     "start_time": "2024-04-07T22:12:25.014831005Z"
    }
   },
   "id": "98b115d0c225d802",
   "execution_count": 318
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def notify_end_of_cell(message=\"Cell execution completed!\"):\n",
    "    os.system(f'notify-send \"Jupyter Cell Notification\" \"{message}\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:27.127351242Z",
     "start_time": "2024-04-07T22:12:27.120177124Z"
    }
   },
   "id": "71db8c4f4cc58692",
   "execution_count": 319
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "m = EncoderDecoder()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:02.735431257Z",
     "start_time": "2024-04-07T22:12:02.709937340Z"
    }
   },
   "id": "a13a2c9eb8ff02d2",
   "execution_count": 316
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[322], line 13\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_iter):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# if iter % eval_interval == 0:\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m#     losses = estimate_loss(m)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;66;03m#     history.append((iter, losses))\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m#     print(f'Iter {iter}, train loss: {losses[\"train\"]:.3f}, val loss: {losses[\"val\"]:.3f}')\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     x, y, x_ans, y_ans \u001B[38;5;241m=\u001B[39m get_batches(questions, answers)\n\u001B[0;32m---> 13\u001B[0m     logits, loss \u001B[38;5;241m=\u001B[39m m(x, y)\n\u001B[1;32m     14\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad(set_to_none\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     15\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[313], line 9\u001B[0m, in \u001B[0;36mEncoderDecoder.forward\u001B[0;34m(self, prompt, response)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, prompt, response):\n\u001B[0;32m----> 9\u001B[0m     k, v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(prompt)\n\u001B[1;32m     11\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(response, k, v)\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[312], line 16\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     14\u001B[0m B, T \u001B[38;5;241m=\u001B[39m idx\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m     15\u001B[0m token_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_embedding_table(idx)\n\u001B[0;32m---> 16\u001B[0m position_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_table(torch\u001B[38;5;241m.\u001B[39marange(T))\n\u001B[1;32m     17\u001B[0m x \u001B[38;5;241m=\u001B[39m token_embedding \u001B[38;5;241m+\u001B[39m position_embedding\n\u001B[1;32m     18\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks(x)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39membedding(\n\u001B[1;32m    164\u001B[0m         \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_idx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_norm,\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm_type, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale_grad_by_freq, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparse)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001B[0m, in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2231\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[1;32m   2232\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[1;32m   2233\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m   2234\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[1;32m   2235\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[1;32m   2236\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[0;32m-> 2237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39membedding(weight, \u001B[38;5;28minput\u001B[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001B[0;31mIndexError\u001B[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.0003)\n",
    "\n",
    "history = []\n",
    "eval_interval = 25\n",
    "max_iter = 1000\n",
    "for iter in range(max_iter):\n",
    "    # if iter % eval_interval == 0:\n",
    "    #     losses = estimate_loss(m)\n",
    "    #     history.append((iter, losses))\n",
    "    #     print(f'Iter {iter}, train loss: {losses[\"train\"]:.3f}, val loss: {losses[\"val\"]:.3f}')\n",
    "\n",
    "    x, y, x_ans, y_ans = get_batches(questions, answers)\n",
    "    logits, loss = m(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "notify_end_of_cell()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T22:12:53.881287248Z",
     "start_time": "2024-04-07T22:12:53.174286456Z"
    }
   },
   "id": "873ab0ac2df82323",
   "execution_count": 322
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "x = [h[0] for h in history]\n",
    "y = [h[1]['train'] for h in history]\n",
    "y_val = [h[1]['val'] for h in history]\n",
    "plt.plot(x, y, label='train')\n",
    "plt.plot(x, y_val, label='val')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-07T22:12:02.813421274Z"
    }
   },
   "id": "11094736f4f7ede6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, 1000)[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-07T22:12:02.813564012Z"
    }
   },
   "id": "c266e5c673669841",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), './models/parallel_checkpoints_64_2.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-07T22:12:02.813615499Z"
    }
   },
   "id": "21360823cf1ce48f",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
